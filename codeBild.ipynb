{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36d96a85-3f81-408c-b2d0-6063d91c97a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Screenshot saved: screenshot_20230707-154107.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]C:\\Users\\ferdi\\anaconda3\\envs\\IoT\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "C:\\Users\\ferdi\\anaconda3\\envs\\IoT\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      anger   disgust      fear  happiness   sadness  surprise   neutral\n",
      "0  0.060625  0.000037  0.000129   0.017403  0.050121  0.004626  0.867059\n",
      "Screenshot saved: screenshot_20230707-154116.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]C:\\Users\\ferdi\\anaconda3\\envs\\IoT\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "C:\\Users\\ferdi\\anaconda3\\envs\\IoT\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      anger   disgust     fear  happiness  sadness  surprise   neutral\n",
      "0  0.028107  0.000051  0.00012   0.006985   0.0221  0.074753  0.867885\n",
      "Screenshot saved: screenshot_20230707-154123.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 118\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Starte das Hauptprogramm\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 108\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    106\u001b[0m region \u001b[38;5;241m=\u001b[39m bring_zoom_meeting_to_front()\n\u001b[0;32m    107\u001b[0m screenshot_name \u001b[38;5;241m=\u001b[39m take_screenshot(region)\n\u001b[1;32m--> 108\u001b[0m emotion \u001b[38;5;241m=\u001b[39m \u001b[43mpyFeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscreenshot_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m visualisieren(emotion)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Überprüfe, ob Escape-Taste gedrückt wurde, um die Schleife zu unterbrechen\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 86\u001b[0m, in \u001b[0;36mpyFeat\u001b[1;34m(screenshot_name)\u001b[0m\n\u001b[0;32m     84\u001b[0m test_data_dir \u001b[38;5;241m=\u001b[39m get_test_data_path()\n\u001b[0;32m     85\u001b[0m bild \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(test_data_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/ferdi/OneDrive/SS23/Iot Python/Projekt/data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscreenshot_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 86\u001b[0m vorhersage \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbild\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Wenn abgespeichert werden soll\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m#pd.DataFrame(vorhersage.emotions).to_csv(f\"C:/Users/ferdi/OneDrive/SS23/Iot Python/Projekt/data/emotion_{screenshot_name}.csv\")      \u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# als Variable weiterverwenden\u001b[39;00m\n\u001b[0;32m     92\u001b[0m emotion \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(vorhersage\u001b[38;5;241m.\u001b[39memotions)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IoT\\lib\\site-packages\\feat\\detector.py:775\u001b[0m, in \u001b[0;36mDetector.detect_image\u001b[1;34m(self, input_file_list, output_size, batch_size, num_workers, pin_memory, frame_counter, face_detection_threshold, **kwargs)\u001b[0m\n\u001b[0;32m    771\u001b[0m batch_output \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_id, batch_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(data_loader)):\n\u001b[1;32m--> 775\u001b[0m     faces, landmarks, poses, aus, emotions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_detection_waterfall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    777\u001b[0m \u001b[43m        \u001b[49m\u001b[43mface_detection_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[43m        \u001b[49m\u001b[43mface_model_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlandmark_model_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfacepose_model_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[43m        \u001b[49m\u001b[43memotion_model_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mau_model_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    785\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_fex(\n\u001b[0;32m    786\u001b[0m         faces,\n\u001b[0;32m    787\u001b[0m         landmarks,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    792\u001b[0m         frame_counter,\n\u001b[0;32m    793\u001b[0m     )\n\u001b[0;32m    794\u001b[0m     batch_output\u001b[38;5;241m.\u001b[39mappend(output)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IoT\\lib\\site-packages\\feat\\detector.py:678\u001b[0m, in \u001b[0;36mDetector._run_detection_waterfall\u001b[1;34m(self, batch_data, face_detection_threshold, face_model_kwargs, landmark_model_kwargs, facepose_model_kwargs, emotion_model_kwargs, au_model_kwargs)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_detection_waterfall\u001b[39m(\n\u001b[0;32m    652\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    653\u001b[0m     batch_data,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    659\u001b[0m     au_model_kwargs,\n\u001b[0;32m    660\u001b[0m ):\n\u001b[0;32m    661\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;124;03m    Main detection \"waterfall.\" Calls each individual detector in the sequence\u001b[39;00m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;124;03m    required to support any interactions between detections. Called\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;124;03m        tuple: faces, landmarks, poses, aus, emotions\u001b[39;00m\n\u001b[0;32m    677\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     faces \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetect_faces(\n\u001b[0;32m    679\u001b[0m         batch_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    680\u001b[0m         threshold\u001b[38;5;241m=\u001b[39mface_detection_threshold,\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mface_model_kwargs,\n\u001b[0;32m    682\u001b[0m     )\n\u001b[0;32m    684\u001b[0m     landmarks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetect_landmarks(\n\u001b[0;32m    685\u001b[0m         batch_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    686\u001b[0m         detected_faces\u001b[38;5;241m=\u001b[39mfaces,\n\u001b[0;32m    687\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlandmark_model_kwargs,\n\u001b[0;32m    688\u001b[0m     )\n\u001b[0;32m    689\u001b[0m     poses_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetect_facepose(\n\u001b[0;32m    690\u001b[0m         batch_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage\u001b[39m\u001b[38;5;124m\"\u001b[39m], landmarks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfacepose_model_kwargs\n\u001b[0;32m    691\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IoT\\lib\\site-packages\\feat\\detector.py:386\u001b[0m, in \u001b[0;36mDetector.detect_faces\u001b[1;34m(self, frame, threshold, **face_model_kwargs)\u001b[0m\n\u001b[0;32m    384\u001b[0m     faces, poses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mface_detector(frame, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mface_model_kwargs)\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 386\u001b[0m     faces \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mface_detector(frame, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mface_model_kwargs)\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_of_lists_empty(faces):\n\u001b[0;32m    389\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: NO FACE is detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IoT\\lib\\site-packages\\feat\\face_detectors\\Retinaface\\Retinaface_test.py:110\u001b[0m, in \u001b[0;36mRetinaface.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    107\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    108\u001b[0m scale \u001b[38;5;241m=\u001b[39m scale\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 110\u001b[0m loc, conf, landms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m    111\u001b[0m total_boxes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(loc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IoT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IoT\\lib\\site-packages\\feat\\face_detectors\\Retinaface\\Retinaface_model.py:281\u001b[0m, in \u001b[0;36mRetinaFace.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m--> 281\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;66;03m# FPN\u001b[39;00m\n\u001b[0;32m    284\u001b[0m     fpn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfpn(out)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IoT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IoT\\lib\\site-packages\\torchvision\\models\\_utils.py:69\u001b[0m, in \u001b[0;36mIntermediateLayerGetter.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     67\u001b[0m out \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 69\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_layers:\n\u001b[0;32m     71\u001b[0m         out_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_layers[name]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IoT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IoT\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IoT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IoT\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IoT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IoT\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IoT\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pyautogui\n",
    "import keyboard\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pygetwindow as gw\n",
    "from feat.utils.io import get_test_data_path\n",
    "from feat.plotting import imshow\n",
    "import os\n",
    "from feat import Detector\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def show_data_window(dataframe):\n",
    "    \n",
    "    plt.close()\n",
    "    plt.ion()\n",
    "    fig = plt.figure(figsize=(8, 2))  # Größe des Fensters anpassen\n",
    "    \n",
    "    fig.canvas.manager.window.attributes('-topmost', True)  # Fenster in den Vordergrund bringen\n",
    "    fig.subplots_adjust(top=1, left=0)\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    ax.clear()\n",
    "    ax.bar(dataframe.index, dataframe)\n",
    "    ax.set_xlabel('Index')\n",
    "    ax.set_ylabel('Werte')\n",
    "    ax.set_title('Index und Werte')\n",
    "    ax.set_xticks(dataframe.index)  # Setzt die x-Achsen-Markierungen auf die Indexwerte\n",
    "    #ax.set_xticklabels(dataframe['Index'])  # Setzt die x-Achsen-Beschriftungen auf die Indexwerte\n",
    "    plt.pause(0.1)\n",
    "\n",
    "        \n",
    "\n",
    "    plt.ioff()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def bring_zoom_meeting_to_front():\n",
    "    # Den Fenstertitel des Zoom-Meetings angeben\n",
    "    zoom_window_title = \"Zoom Meeting\"\n",
    "    \n",
    "    # Das Zoom-Meeting-Fenster suchen und in den Vordergrund bringen\n",
    "    zoom_window = gw.getWindowsWithTitle(zoom_window_title)[0]\n",
    "    #zoom_window.restore()  # Das Fenster wiederherstellen, falls es minimiert ist\n",
    "    #zoom_window.maximize()  # Das Fenster maximieren\n",
    "    \n",
    "    # Die Position und Größe des Zoom-Meeting-Fensters abrufen\n",
    "    left, top, right, bottom = zoom_window.left, zoom_window.top, zoom_window.right, zoom_window.bottom\n",
    "    \n",
    "    return left, top, right, bottom\n",
    "\n",
    "\n",
    "def take_screenshot(region):\n",
    "    # Aktuellen Zeitstempel für den Dateinamen generieren\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\", time.localtime())\n",
    "\n",
    "    # Screenshot des ausgewählten Bereichs aufnehmen\n",
    "    screenshot = pyautogui.screenshot(region=region)\n",
    "      # Dateipfad und Name für den Screenshot\n",
    "    #screenshot_path = f\"./data/screenshot_{timestamp}.png\"\n",
    "    screenshot_name = f\"screenshot_{timestamp}\"\n",
    "    # Speichern des Screenshots\n",
    "    screenshot.save(f\"./data/{screenshot_name}.png\")\n",
    "    print(f\"Screenshot saved: screenshot_{timestamp}.png\")\n",
    "    return screenshot_name\n",
    "\n",
    "def pyFeat(screenshot_name):\n",
    "    detector = Detector( \n",
    "\tface_model=\"retinaface\",\n",
    "\tlandmark_model=\"mobilefacenet\",\n",
    "\tau_model='xgb',\n",
    "\temotion_model=\"resmasknet\",\n",
    "    Facepose_model=\"img2pose\",)\n",
    "    \n",
    "    test_data_dir = get_test_data_path()\n",
    "    bild = os.path.join(test_data_dir, f\"C:/Users/ferdi/OneDrive/SS23/Iot Python/Projekt/data/{screenshot_name}.png\")\n",
    "    vorhersage = detector.detect_image(bild)\n",
    "    \n",
    "    \n",
    "    # Wenn abgespeichert werden soll\n",
    "    #pd.DataFrame(vorhersage.emotions).to_csv(f\"C:/Users/ferdi/OneDrive/SS23/Iot Python/Projekt/data/emotion_{screenshot_name}.csv\")      \n",
    "    # als Variable weiterverwenden\n",
    "    emotion = pd.DataFrame(vorhersage.emotions)\n",
    "    return emotion\n",
    "\n",
    "def visualisieren(emotion):\n",
    "    df = emotion.mean()\n",
    "    show_data_window(df)\n",
    "    print(emotion)\n",
    "    \n",
    "    #create_gui(mean)\n",
    "    # Hauptprogramm\n",
    "def main():\n",
    "    # Endlosschleife, um den Screenshot alle 3 Sekunden zu machen\n",
    "    while True:\n",
    "        # Screenshot aufnehmen und speichern\n",
    "        region = bring_zoom_meeting_to_front()\n",
    "        screenshot_name = take_screenshot(region)\n",
    "        emotion = pyFeat(screenshot_name)\n",
    "        visualisieren(emotion)\n",
    "       \n",
    "        # Überprüfe, ob Escape-Taste gedrückt wurde, um die Schleife zu unterbrechen\n",
    "        if keyboard.is_pressed(\"esc\"):\n",
    "            break\n",
    "        \n",
    "\n",
    "\n",
    "# Starte das Hauptprogramm\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea3b246-1bc5-4d11-91f1-bde4a6aa91ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
